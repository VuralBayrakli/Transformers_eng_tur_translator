{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff85c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
    "from collections import Counter \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d39475",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc10313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    '''\n",
    "    Prepare English and French examples for batch-friendly modeling by appending\n",
    "    BOS/EOS tokens to each, stacking the tensors, and filling trailing spaces of\n",
    "    shorter sentences with the <pad> token. To be used as the collate_fn in the\n",
    "    English-to-Turkish DataLoader.\n",
    "\n",
    "    Input: \n",
    "    - data_batch, an iterable of (English, Turkish) tuples from the datasets \n",
    "      created above\n",
    "\n",
    "    Outputs\n",
    "    - en_batch: a (max length X batch size) tensor of English token IDs\n",
    "    - tr_batch: a (max length X batch size) tensor of Turkish token IDs \n",
    "    '''\n",
    "    en_batch, tr_batch = [], []\n",
    "    for (en_item, tr_item) in data_batch:\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        tr_batch.append(torch.cat([torch.tensor([BOS_IDX]), tr_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    tr_batch = pad_sequence(tr_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "\n",
    "    return en_batch, tr_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b630a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Değişkeni geri yükleme\n",
    "with open('en_vocab.pkl', 'rb') as f:\n",
    "    en_vocab = pickle.load(f)\n",
    "    \n",
    "with open('tr_vocab.pkl', 'rb') as f:\n",
    "    tr_vocab = pickle.load(f)\n",
    "    \n",
    "PAD_IDX = en_vocab['<pad>']\n",
    "BOS_IDX = en_vocab['<bos>']\n",
    "EOS_IDX = en_vocab['<eos>']\n",
    "\n",
    "SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for en_id, tr_id in zip(en_vocab.lookup_indices(SPECIALS), tr_vocab.lookup_indices(SPECIALS)):\n",
    "    assert en_id == tr_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13f46a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: <bos> don t push me . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "French: <bos> beni itme . <eos> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "English: <bos> tell tom to pick up the phone . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "French: <bos> tom a telefonu açmasını söyle . <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "English: <bos> aren t you ready ? <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "French: <bos> hazır değil misiniz ? <eos> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "English: <bos> can you show me how to do it ? <eos> <pad>\n",
      "French: <bos> bana onu nasıl yapacağımı gösterebilir misin ? <eos> <pad> <pad>\n",
      "\n",
      "English: <bos> tom wants to become a surgeon . <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "French: <bos> tom bir cerrah olmak istiyor . <eos> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Değişkeni geri yükleme\n",
    "with open('train_iter.pkl', 'rb') as f:\n",
    "    train_iter = pickle.load(f)\n",
    "\n",
    "with open('en_vocab.pkl', 'rb') as f:\n",
    "    en_vocab = pickle.load(f)    \n",
    "    # Değişkeni geri yükleme\n",
    "with open('tr_vocab.pkl', 'rb') as f:\n",
    "    tr_vocab = pickle.load(f)\n",
    "    \n",
    "with open('test_iter.pkl', 'rb') as f:\n",
    "    test_iter = pickle.load(f)\n",
    "    \n",
    "with open('valid_iter.pkl', 'rb') as f:\n",
    "    valid_iter = pickle.load(f)\n",
    "    \n",
    "    \n",
    "for i, (en_id, tr_id) in enumerate(train_iter):\n",
    "    print('English:', ' '.join([en_vocab.lookup_token(idx) for idx in en_id[:, 0]]))\n",
    "    print('French:', ' '.join([tr_vocab.lookup_token(idx) for idx in tr_id[:, 0]]))\n",
    "    if i == 4: \n",
    "        break\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae184ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttentionQKV(nn.Module):\n",
    "    def __init__(self, hidden_size, query_size=None, key_size=None, dropout_p=0.15):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.query_size = hidden_size if query_size is None else query_size\n",
    "        \n",
    "        # assume bidirectional encoder, but can specify otherwise\n",
    "        self.key_size = 2*hidden_size if key_size is None else key_size\n",
    "        \n",
    "        self.query_layer = nn.Linear(self.query_size, hidden_size)\n",
    "        self.key_layer = nn.Linear(self.key_size, hidden_size)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, src_mask=None):\n",
    "        '''\n",
    "        Calculate attention weights using query and key features, with\n",
    "        an optional mask for input sequences.\n",
    "        \n",
    "        Inputs:\n",
    "          - hidden: most recent RNN hidden state; (B, Dec)\n",
    "          - encoder_outputs: RNN outputs at individual time steps with \n",
    "            directions concatenated; (L, B, 2*Enc)\n",
    "          - src_mask: boolean tensor of same size as source tokens (Src, B) \n",
    "            where False denotes tokens to be ignored\n",
    "            \n",
    "        Outputs:\n",
    "          - attention weights: (B, src) tensor of softmax attention \n",
    "            weights to be applied to downstream values\n",
    "        '''\n",
    "        \n",
    "        # (B, H)\n",
    "        query_out = self.query_layer(hidden) \n",
    "        \n",
    "        # (Src, B, 2*H) --> (Src, B, H)\n",
    "        key_out = self.key_layer(encoder_outputs) \n",
    "        \n",
    "        # (B, H) + (Src, B, H) = (Src, B, H)\n",
    "        energy_input = torch.tanh(query_out + key_out) \n",
    "        \n",
    "        # (Src, B, H) --> (Src, B, 1) --> (Src, B)\n",
    "        energies = self.energy_layer(energy_input).squeeze(2) \n",
    "        \n",
    "        # if a mask is provided, remove masked tokens from softmax calc\n",
    "        if src_mask is not None:\n",
    "            energies.data.masked_fill_(src_mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        # softmax over the length dimension\n",
    "        weights = F.softmax(energies, dim=0)\n",
    "        \n",
    "        # return as (B, Src) as expected by later multiplication\n",
    "        return weights.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9a90404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout_p=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, num_attention_heads, \n",
    "                 num_encoder_layers, num_decoder_layers, dim_feedforward, \n",
    "                 max_seq_length, pos_dropout, transformer_dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed_src = nn.Embedding(input_dim, d_model)\n",
    "        self.embed_tgt = nn.Embedding(output_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model, num_attention_heads, num_encoder_layers, \n",
    "                                          num_decoder_layers, dim_feedforward, transformer_dropout)\n",
    "        self.output = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self,\n",
    "                src=None, \n",
    "                tgt=None,\n",
    "                src_mask=None,\n",
    "                tgt_mask=None, \n",
    "                src_key_padding_mask=None, \n",
    "                tgt_key_padding_mask=None,\n",
    "                memory_key_padding_mask=None,\n",
    "                src_embeds=None, \n",
    "                tgt_embeds=None):\n",
    "        \n",
    "        if (src_embeds is None) and (src is not None):\n",
    "            if (tgt_embeds is None) and (tgt is not None):\n",
    "                src_embeds, tgt_embeds = self._embed_tokens(src, tgt)\n",
    "        elif (src_embeds is not None) and (src is not None):\n",
    "            raise ValueError(\"Must specify exactly one of src and src_embeds\")\n",
    "        elif (src_embeds is None) and (src is None):\n",
    "            raise ValueError(\"Must specify exactly one of src and src_embeds\")\n",
    "        elif (tgt_embeds is not None) and (tgt is not None):\n",
    "            raise ValueError(\"Must specify exactly one of tgt and tgt_embeds\")\n",
    "        elif (tgt_embeds is None) and (tgt is None):\n",
    "            raise ValueError(\"Must specify exactly one of tgt and tgt_embeds\")\n",
    "        \n",
    "        output = self.transformer(src_embeds, \n",
    "                                  tgt_embeds, \n",
    "                                  tgt_mask=tgt_mask, \n",
    "                                  src_key_padding_mask=src_key_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        return self.output(output)\n",
    "    \n",
    "    def _embed_tokens(self, src, tgt):\n",
    "        src_embeds = self.embed_src(src) * np.sqrt(self.d_model)\n",
    "        tgt_embeds = self.embed_tgt(tgt) * np.sqrt(self.d_model)\n",
    "        \n",
    "        src_embeds = self.pos_enc(src_embeds)\n",
    "        tgt_embeds = self.pos_enc(tgt_embeds)\n",
    "        return src_embeds, tgt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d74e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, iterator, optimizer, loss_fn, device, clip=None):\n",
    "    model.train()\n",
    "        \n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(iterator), leave=False) as t:\n",
    "        for i, (src, tgt) in enumerate(iterator):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            # Create tgt_inp and tgt_out (which is tgt_inp but shifted by 1)\n",
    "            tgt_inp, tgt_out = tgt[:-1, :], tgt[1:, :]\n",
    "\n",
    "            tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_inp.size(0)).to(device)\n",
    "            src_key_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "            tgt_key_padding_mask = (tgt_inp == PAD_IDX).transpose(0, 1)\n",
    "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(src=src, tgt=tgt_inp, \n",
    "                           tgt_mask=tgt_mask,\n",
    "                           src_key_padding_mask = src_key_padding_mask,\n",
    "                           tgt_key_padding_mask = tgt_key_padding_mask,\n",
    "                           memory_key_padding_mask = memory_key_padding_mask)\n",
    "            \n",
    "            loss = loss_fn(output.view(-1, output.shape[2]),\n",
    "                           tgt_out.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / (i+1)\n",
    "            t.set_postfix(loss='{:05.3f}'.format(avg_loss),\n",
    "                          ppl='{:05.3f}'.format(np.exp(avg_loss)))\n",
    "            t.update()\n",
    "            \n",
    "    return epoch_loss / len(iterator)\n",
    "    \n",
    "def evaluate_transformer(model, iterator, loss_fn, device):\n",
    "    model.eval()\n",
    "        \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(iterator), leave=False) as t:\n",
    "            for i, (src, tgt) in enumerate(iterator):\n",
    "                src = src.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "                \n",
    "                # Create tgt_inp and tgt_out (which is tgt_inp but shifted by 1)\n",
    "                tgt_inp, tgt_out = tgt[:-1, :], tgt[1:, :]\n",
    "                \n",
    "                tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_inp.size(0)).to(device)\n",
    "                src_key_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "                tgt_key_padding_mask = (tgt_inp == PAD_IDX).transpose(0, 1)\n",
    "                memory_key_padding_mask = src_key_padding_mask.clone()\n",
    "\n",
    "                output = model(src=src, tgt=tgt_inp, \n",
    "                               tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask = src_key_padding_mask,\n",
    "                               tgt_key_padding_mask = tgt_key_padding_mask,\n",
    "                               memory_key_padding_mask = memory_key_padding_mask)\n",
    "                \n",
    "                loss = loss_fn(output.view(-1, output.shape[2]),\n",
    "                               tgt_out.view(-1))\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                avg_loss = epoch_loss / (i+1)\n",
    "                t.set_postfix(loss='{:05.3f}'.format(avg_loss),\n",
    "                              ppl='{:05.3f}'.format(np.exp(avg_loss)))\n",
    "                t.update()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d867149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerModel(input_dim=len(en_vocab), \n",
    "                             output_dim=len(tr_vocab), \n",
    "                             d_model=256, \n",
    "                             num_attention_heads=8,\n",
    "                             num_encoder_layers=6, \n",
    "                             num_decoder_layers=6, \n",
    "                             dim_feedforward=2048,\n",
    "                             max_seq_length=32,\n",
    "                             pos_dropout=0.15,\n",
    "                             transformer_dropout=0.3)\n",
    "\n",
    "transformer = transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69e85a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "xf_optim = torch.optim.AdamW(transformer.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "753eb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfdd09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e476ac6689154f0da9af4a0b05bda57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e129b5971bad43be9f1252d00714fe10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VuralBayrakli\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N_EPOCHS = 50\n",
    "CLIP = 15 # clipping value, or None to prevent gradient clipping\n",
    "EARLY_STOPPING_EPOCHS = 5\n",
    "SAVE_DIR = os.getcwd() \n",
    "model_path = os.path.join(SAVE_DIR, 'transformer_en_tr.pt')\n",
    "transformer_metrics = {}\n",
    "best_valid_loss = float(\"inf\")\n",
    "early_stopping_count = 0\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Epoch\"):\n",
    "    train_loss = train_transformer(transformer, train_iter, xf_optim, loss_fn, device, clip=CLIP)\n",
    "    valid_loss = evaluate_transformer(transformer, valid_iter, loss_fn, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        tqdm.write(f\"Checkpointing at epoch {epoch + 1}\")\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(transformer.state_dict(), model_path)\n",
    "        early_stopping_count = 0\n",
    "    elif epoch > EARLY_STOPPING_EPOCHS:\n",
    "        early_stopping_count += 1\n",
    "    \n",
    "    transformer_metrics[epoch+1] = dict(\n",
    "        train_loss = train_loss,\n",
    "        train_ppl = np.exp(train_loss),\n",
    "        valid_loss = valid_loss,\n",
    "        valid_ppl = np.exp(valid_loss)\n",
    "    )\n",
    "    \n",
    "    if early_stopping_count == EARLY_STOPPING_EPOCHS:\n",
    "        tqdm.write(f\"Early stopping triggered in epoch {epoch + 1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_transformer(text, model, \n",
    "                        src_vocab=en_vocab, \n",
    "                        src_tokenizer=en_tokenizer, \n",
    "                        tgt_vocab=fr_vocab, \n",
    "                        device=device):\n",
    "    \n",
    "    input_ids = [src_vocab[token] for token in src_tokenizer(text)]\n",
    "    input_ids = [BOS_IDX] + input_ids + [EOS_IDX]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(input_ids).to(device).unsqueeze(1) \n",
    "        \n",
    "        causal_out = torch.ones(MAX_SENTENCE_LENGTH, 1).long().to(device) * BOS_IDX\n",
    "        for t in range(1, MAX_SENTENCE_LENGTH):\n",
    "            decoder_output = transformer(input_tensor, causal_out[:t, :])[-1, :, :]\n",
    "            next_token = decoder_output.data.topk(1)[1].squeeze()\n",
    "            causal_out[t, :] = next_token\n",
    "            if next_token.item() == EOS_IDX:\n",
    "                break\n",
    "                \n",
    "        pred_words = [tgt_vocab.lookup_token(tok.item()) for tok in causal_out.squeeze(1)[1:(t)]]\n",
    "        return \" \".join(pred_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_transformer(\"she is not my mother .\", transformer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
